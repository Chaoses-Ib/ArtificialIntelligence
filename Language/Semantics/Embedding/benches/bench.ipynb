{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[570]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_used_vram():\n",
    "    command = \"nvidia-smi --query-gpu=memory.used --format=csv\"\n",
    "    memory_free_info = subprocess.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "get_used_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.08 GiB | 0.79 GiB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "base_ws = None\n",
    "def print_mem():\n",
    "    global base_ws\n",
    "\n",
    "    process = psutil.Process()\n",
    "    ws = process.memory_info().rss\n",
    "    # if base_ws is None:\n",
    "    #     base_ws = ws\n",
    "    #     print(f'Base WS: {base_ws / (1 << 30):.2f} GiB')\n",
    "    # else:\n",
    "    #     ws -= base_ws\n",
    "\n",
    "    vram = get_used_vram()\n",
    "\n",
    "    print(f'Used ws and vram: {ws / (1 << 30):.2f} GiB | {vram[0] / 1024:.2f} GiB')\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Used ws and vram: 0.42 GiB | 0.81 GiB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148574\n",
      "298\n",
      "Used ws and vram: 0.08 GiB | 0.78 GiB\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "\n",
    "chunks = data.chunks()\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.52 GiB | 0.98 GiB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from numpy.linalg import norm\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 4.58 GiB | 0.94 GiB\n"
     ]
    }
   ],
   "source": [
    "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
    "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-zh', trust_remote_code=True) # trust_remote_code is needed to use the encode method\n",
    "# embeddings = model.encode(['How is the weather today a?', '今天天气怎么样?'])\n",
    "# print(cos_sim(embeddings[0], embeddings[1]))\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.52 GiB | 0.98 GiB\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2', trust_remote_code=True) # trust_remote_code is needed to use the encode method\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def encode(sentences):\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "# embeddings = model.encode(['How is the weather today a?', '今天天气怎么样?'])\n",
    "# print(cos_sim(embeddings[0], embeddings[1]))\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.57 GiB | 0.98 GiB\n"
     ]
    }
   ],
   "source": [
    "embeddings = encode(['How is the weather today a?', '今天天气怎么样?'])\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.4 ms ± 9.43 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model.encode(['How is the weather today a?', '今天天气怎么样?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.48 s ± 481 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "samples = chunks[:256]\n",
    "%timeit encode(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used memory and vram: 0.97 GiB | 0.92 GiB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Home\\计算机.AI\\public\\Language\\Semantics\\Embedding\\benches\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.58 GiB | 0.78 GiB\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy.linalg import norm\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.58 GiB | 1.09 GiB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 1.20 GiB | 4.69 GiB\n"
     ]
    }
   ],
   "source": [
    "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
    "model = SentenceTransformer('jinaai/jina-embeddings-v2-base-zh', device='cuda:0', trust_remote_code=True)\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(['How is the weather today?', '今天天气怎么样?'])\n",
    "print(cos_sim(embeddings[0], embeddings[1]))\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.64 GiB | 1.16 GiB\n",
      "Used ws and vram: 1.72 GiB | 1.00 GiB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "print_mem()\n",
    "\n",
    "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
    "model = SentenceTransformer('jinaai/jina-embeddings-v2-base-zh', device='cpu', backend='onnx', trust_remote_code=True, model_kwargs={\n",
    "    'file_name': 'onnx/model.onnx'\n",
    "})\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.59 GiB | 0.86 GiB\n",
      "Used ws and vram: 0.90 GiB | 1.08 GiB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "print_mem()\n",
    "\n",
    "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
    "model = SentenceTransformer('jinaai/jina-embeddings-v2-base-zh', device='cpu', backend='onnx', trust_remote_code=True, model_kwargs={\n",
    "    'file_name': 'onnx/model_quantized.onnx'\n",
    "})\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.46 s ± 88 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Used ws and vram: 1.84 GiB | 1.01 GiB\n"
     ]
    }
   ],
   "source": [
    "samples = chunks[:64]\n",
    "%timeit model.encode(samples, batch_size=32)\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 2.20 GiB | 0.89 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.models.Transformer:Multiple ONNX files found in 'jinaai/jina-embeddings-v2-base-zh': ['onnx/model.onnx', 'onnx/model_fp16.onnx', 'onnx/model_quantized.onnx'], defaulting to 'onnx/model.onnx'. Please specify the desired file name via `model_kwargs={\"file_name\": \"<file_name>\"}`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 3.27 GiB | 0.89 GiB\n",
      "Used ws and vram: 3.28 GiB | 0.88 GiB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "print_mem()\n",
    "\n",
    "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
    "model = SentenceTransformer('jinaai/jina-embeddings-v2-base-zh', device='cpu', backend='onnx', trust_remote_code=True)\n",
    "print_mem()\n",
    "\n",
    "from sentence_transformers import export_dynamic_quantized_onnx_model\n",
    "\n",
    "export_dynamic_quantized_onnx_model(model, \"avx512_vnni\", \"private/jina-embeddings-v2-base-zh\")\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx/model_qint8_avx512.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file model_qint8_avx512.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.92 GiB | 0.78 GiB\n",
      "4.84 s ± 340 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Used ws and vram: 1.49 GiB | 0.78 GiB\n",
      "\n",
      "onnx/model_qint8_avx512_vnni.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file model_qint8_avx512_vnni.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.98 GiB | 0.78 GiB\n",
      "5.12 s ± 428 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Used ws and vram: 1.50 GiB | 0.78 GiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
    "# Multiple ONNX files found in 'sentence-transformers/all-MiniLM-L6-v2': ['onnx/model.onnx', 'onnx/model_O1.onnx', 'onnx/model_O2.onnx', 'onnx/model_O3.onnx', 'onnx/model_O4.onnx', 'onnx/model_qint8_arm64.onnx', 'onnx/model_qint8_avx512.onnx', 'onnx/model_qint8_avx512_vnni.onnx', 'onnx/model_quint8_avx2.onnx'], defaulting to 'onnx/model.onnx'. Please specify the desired file name via `model_kwargs={\"file_name\": \"<file_name>\"}`.\n",
    "for m in ['onnx/model_qint8_avx512.onnx', 'onnx/model_qint8_avx512_vnni.onnx']:\n",
    "    print(m)\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    model = SentenceTransformer('private/jina-embeddings-v2-base-zh', device='cpu', backend='onnx', model_kwargs={\n",
    "        'file_name': m\n",
    "    })\n",
    "    print_mem()\n",
    "\n",
    "    samples = chunks[:64]\n",
    "    %timeit model.encode(samples, batch_size=32)\n",
    "    print_mem()\n",
    "    print()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx/model.onnx\n",
      "Used ws and vram: 0.75 GiB | 0.80 GiB\n",
      "1.18 s ± 170 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Used ws and vram: 0.94 GiB | 0.80 GiB\n",
      "\n",
      "onnx/model_O1.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file model_O1.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.79 GiB | 0.80 GiB\n",
      "1.32 s ± 246 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Used ws and vram: 0.93 GiB | 0.80 GiB\n",
      "\n",
      "onnx/model_O2.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file model_O2.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.79 GiB | 0.80 GiB\n",
      "1.08 s ± 108 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Used ws and vram: 0.96 GiB | 0.80 GiB\n",
      "\n",
      "onnx/model_O3.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file model_O3.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.79 GiB | 0.80 GiB\n",
      "1.14 s ± 53.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Used ws and vram: 0.95 GiB | 0.80 GiB\n",
      "\n",
      "onnx/model_qint8_arm64.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file model_qint8_arm64.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.70 GiB | 0.80 GiB\n",
      "867 ms ± 63.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Used ws and vram: 0.91 GiB | 0.80 GiB\n",
      "\n",
      "onnx/model_qint8_avx512.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file model_qint8_avx512.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.70 GiB | 0.80 GiB\n",
      "929 ms ± 104 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Used ws and vram: 0.90 GiB | 0.80 GiB\n",
      "\n",
      "onnx/model_qint8_avx512_vnni.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file model_qint8_avx512_vnni.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.70 GiB | 0.80 GiB\n",
      "792 ms ± 68 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Used ws and vram: 0.90 GiB | 0.80 GiB\n",
      "\n",
      "onnx/model_quint8_avx2.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file model_quint8_avx2.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.70 GiB | 0.80 GiB\n",
      "865 ms ± 72.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Used ws and vram: 0.91 GiB | 0.80 GiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
    "# Multiple ONNX files found in 'sentence-transformers/all-MiniLM-L6-v2': ['onnx/model.onnx', 'onnx/model_O1.onnx', 'onnx/model_O2.onnx', 'onnx/model_O3.onnx', 'onnx/model_O4.onnx', 'onnx/model_qint8_arm64.onnx', 'onnx/model_qint8_avx512.onnx', 'onnx/model_qint8_avx512_vnni.onnx', 'onnx/model_quint8_avx2.onnx'], defaulting to 'onnx/model.onnx'. Please specify the desired file name via `model_kwargs={\"file_name\": \"<file_name>\"}`.\n",
    "for m in ['onnx/model.onnx', 'onnx/model_O1.onnx', 'onnx/model_O2.onnx', 'onnx/model_O3.onnx', 'onnx/model_qint8_arm64.onnx', 'onnx/model_qint8_avx512.onnx', 'onnx/model_qint8_avx512_vnni.onnx', 'onnx/model_quint8_avx2.onnx']:\n",
    "    print(m)\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu', backend='onnx', model_kwargs={\n",
    "        'file_name': m\n",
    "    })\n",
    "    print_mem()\n",
    "\n",
    "    samples = chunks[:64]\n",
    "    %timeit model.encode(samples, batch_size=32)\n",
    "    print_mem()\n",
    "    print()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5 ms ± 2.46 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "samples = ['How is the weather today?', '今天天气怎么样?']\n",
    "%timeit model.encode(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34 s ± 76.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "samples = chunks[:64]\n",
    "%timeit model.encode(samples, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.49 GiB | 0.71 GiB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-zh and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "100%|██████████| 60/60 [00:11<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9283, 0.0462],\n",
      "        [0.4067, 0.2280]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntensor([[0.8430, 0.3271],\\n        [0.3213, 0.5861]])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: `pip install model2vec` is needed, but not for inference\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.models import StaticEmbedding\n",
    "\n",
    "# Initialize a Sentence Transformer model with a static embedding by distilling via model2vec\n",
    "static_embedding = StaticEmbedding.from_distillation(\n",
    "    \"jinaai/jina-embeddings-v2-base-zh\",\n",
    "    device=\"cuda\",\n",
    "    pca_dims=256,\n",
    "    apply_zipf=True,\n",
    ")\n",
    "model = SentenceTransformer(modules=[static_embedding], backend='onnx', device='cpu', trust_remote_code=True)\n",
    "\n",
    "# Encode some texts\n",
    "queries = [\"What is the capital of France?\", \"How many people live in the Netherlands?\"]\n",
    "documents = [\"Paris is the capital of France\", \"The Netherlands has 17 million inhabitants\"]\n",
    "query_embeddings = model.encode(queries)\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "# Compute similarities\n",
    "scores = model.similarity(query_embeddings, document_embeddings)\n",
    "print(scores)\n",
    "\"\"\"\n",
    "tensor([[0.8430, 0.3271],\n",
    "        [0.3213, 0.5861]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('private/onnx/model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Home\\计算机.AI\\public\\Language\\Semantics\\Embedding\\benches\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: `pip install model2vec` is needed, but not for inference\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.models import StaticEmbedding\n",
    "\n",
    "# Initialize a Sentence Transformer model with a static embedding by distilling via model2vec\n",
    "# static_embedding = StaticEmbedding.from_model2vec(\n",
    "#     'onnx/model.onnx'\n",
    "# )\n",
    "model = SentenceTransformer('private/onnx/model.onnx', backend='onnx', device='cpu', trust_remote_code=True)\n",
    "\n",
    "# Encode some texts\n",
    "queries = [\"What is the capital of France?\", \"How many people live in the Netherlands?\"]\n",
    "documents = [\"Paris is the capital of France\", \"The Netherlands has 17 million inhabitants\"]\n",
    "query_embeddings = model.encode(queries)\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "# Compute similarities\n",
    "scores = model.similarity(query_embeddings, document_embeddings)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.9 ms ± 362 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Used ws and vram: 0.67 GiB | 0.42 GiB\n"
     ]
    }
   ],
   "source": [
    "samples = chunks[:64]\n",
    "%timeit model.encode(samples, batch_size=32)\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used ws and vram: 0.67 GiB | 0.42 GiB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "print_mem()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
