# Datasets
## Chinese
- [zhwiki dump](https://dumps.wikimedia.org/zhwiki/20221201/)

  [Mirrors of XML dumps, images and other data](https://dumps.wikimedia.org/mirrors.html)

  约 45% 的文档长度小于 100 字符。

  [WikiExtractor: A tool for extracting plain text from Wikipedia dumps](https://github.com/attardi/wikiextractor/)
  - [维基百科简体中文语料的提取 - 知乎](https://zhuanlan.zhihu.com/p/39960476)

  [mattzheng/ChineseWiki: 维基百科中文语料整理](https://github.com/mattzheng/ChineseWiki)
- [搜狗实验室数据资源](https://web.archive.org/web/20220702234429/http://www.sogou.com/labs/resource/list_pingce.php)
  - 评测集合
    - [搜索结果评价（SogouE）](https://web.archive.org/web/20211022223156/http://www.sogou.com/labs/resource/e.php)
  - 新闻数据
    - [全网新闻数据（SogouCA）](https://web.archive.org/web/20220322033048/https://www.sogou.com/labs/resource/ca.php)
    - [搜狐新闻数据（SogouCS）](https://web.archive.org/web/20220503011135/https://www.sogou.com/labs/resource/cs.php)
- [Baseline for Chinese Natural Language Inference (CNLI) dataset](https://github.com/blcunlp/CNLI)

### Information sources
- [互联网上开放的中文语料库有哪些？ - 知乎](https://www.zhihu.com/question/21177095)
- [常用数据集简介 · duoergun0729/nlp](https://github.com/duoergun0729/nlp/blob/master/%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E7%AE%80%E4%BB%8B.md)
